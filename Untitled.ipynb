{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73999e2a-4bef-4c5a-868b-62bcb27bb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0885e33f-6f1b-43d5-8332-9e1fbccbd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.txt',sep=' ',header=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac76d0e-5814-4a32-8324-a6d9016e2352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADE_train_00000001</td>\n",
       "      <td>airport_terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADE_train_00000002</td>\n",
       "      <td>airport_terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADE_train_00000003</td>\n",
       "      <td>art_gallery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADE_train_00000004</td>\n",
       "      <td>badlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE_train_00000005</td>\n",
       "      <td>ball_pit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                 1\n",
       "0  ADE_train_00000001  airport_terminal\n",
       "1  ADE_train_00000002  airport_terminal\n",
       "2  ADE_train_00000003       art_gallery\n",
       "3  ADE_train_00000004          badlands\n",
       "4  ADE_train_00000005          ball_pit"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7033d56-c302-41e8-a0db-7bf93bd20edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_file(file_path):\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
    "    return df[\"image\"].tolist(), df[\"category\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6621101c-e698-4204-83b6-dc04187dd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\1864165516.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "image, cat=parse_test_file('data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ec8140-0238-448f-85b8-19cbea5792fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_categories_file(file_path):\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True)\n",
    "    return df[\"Name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aedb87d2-617b-4f0b-8438-9510ed412214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\1864165516.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "test_image_names, test_categories = parse_test_file('data/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4474c649-39fc-4272-baba-d6798db7f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed test images: ['ADE_test_00000001.jpg', 'ADE_test_00000002.jpg', 'ADE_test_00000003.jpg', 'ADE_test_00000005.jpg', 'ADE_test_00000006.jpg']\n",
      "Parsed train images: ['ADE_train_00000001', 'ADE_train_00000002', 'ADE_train_00000003', 'ADE_train_00000004', 'ADE_train_00000005']\n",
      "Parsed train categories: ['airport_terminal', 'airport_terminal', 'art_gallery', 'badlands', 'ball_pit']\n",
      "Parsed val images: ['ADE_val_00000001', 'ADE_val_00000002', 'ADE_val_00000003', 'ADE_val_00000004', 'ADE_val_00000005']\n",
      "Parsed val categories: ['abbey', 'abbey', 'access_road', 'airfield', 'airplane_cabin']\n",
      "Parsed categories list: ['wall', 'building, edifice', 'sky', 'floor, flooring', 'tree']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_test_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, names=[\"image\"])\n",
    "        return df[\"image\"].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_train_val_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
    "        return df[\"image\"].tolist(), df[\"category\"].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def read_categories_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t', engine='python')  # Use tab as delimiter and the Python engine for better handling\n",
    "        return df[\"Name\"].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "test_images = parse_test_file('data/test.txt')\n",
    "train_images, train_categories = parse_train_val_file('data/train.txt')\n",
    "val_images, val_categories = parse_train_val_file('data/val.txt')\n",
    "categories_list = read_categories_file('data/categories.txt')\n",
    "\n",
    "print(\"Parsed test images:\", test_images[:5])  # Print first 5 entries for verification\n",
    "print(\"Parsed train images:\", train_images[:5])\n",
    "print(\"Parsed train categories:\", train_categories[:5])\n",
    "print(\"Parsed val images:\", val_images[:5])\n",
    "print(\"Parsed val categories:\", val_categories[:5])\n",
    "print(\"Parsed categories list:\", categories_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e6753e5-80c2-4183-8ee2-a60a55383376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, image_list, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_list = image_list\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        mask_name = image_name.replace('.jpg', '.png')\n",
    "        mask_path = os.path.join(self.annotations_dir, mask_name)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Example usage for training and validation datasets\n",
    "train_dataset = SegmentationDataset(image_dir='data/images/training',\n",
    "                                    annotations_dir='data/annotations/training',\n",
    "                                    image_list=train_images,\n",
    "                                    transform=None)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir='data/images/validation',\n",
    "                                  annotations_dir='data/annotations/validation',\n",
    "                                  image_list=val_images,\n",
    "                                  transform=None)\n",
    "\n",
    "test_dataset = SegmentationDataset(image_dir='data/testing',\n",
    "                                   annotations_dir='data/annotations/validation',  # Adjust if needed\n",
    "                                   image_list=test_images,\n",
    "                                   transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6521dc0-f417-4021-bd54-fddb8d52c375",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\trish\\\\OneDrive\\\\Desktop\\\\Segmentation_project\\\\data\\\\images\\\\training\\\\ADE_train_00012076'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training loop code here\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Validation loop code here\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[35], line 22\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m mask_name \u001b[38;5;241m=\u001b[39m image_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations_dir, mask_name)\n\u001b[1;32m---> 22\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\trish\\\\OneDrive\\\\Desktop\\\\Segmentation_project\\\\data\\\\images\\\\training\\\\ADE_train_00012076'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    # Training loop code here\n",
    "    pass\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    # Validation loop code here\n",
    "    pass\n",
    "\n",
    "for images, masks in test_loader:\n",
    "    # Testing loop code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb6ddb6e-f52f-4129-b67a-f769ae835875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, image_list, image_ext='.jpg', mask_ext='.png', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_list = image_list\n",
    "        self.image_ext = image_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name + self.image_ext)\n",
    "        mask_path = os.path.join(self.annotations_dir, image_name + self.mask_ext)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Example usage for training and validation datasets\n",
    "train_images, train_categories = parse_train_val_file('data/train.txt')\n",
    "val_images, val_categories = parse_train_val_file('data/val.txt')\n",
    "test_images = parse_test_file('data/test.txt')\n",
    "\n",
    "train_dataset = SegmentationDataset(image_dir='data/images/training',\n",
    "                                    annotations_dir='data/annotations/training',\n",
    "                                    image_list=train_images,\n",
    "                                    image_ext='.jpg',\n",
    "                                    mask_ext='.png',\n",
    "                                    transform=None)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir='data/images/validation',\n",
    "                                  annotations_dir='data/annotations/validation',\n",
    "                                  image_list=val_images,\n",
    "                                  image_ext='.jpg',\n",
    "                                  mask_ext='.png',\n",
    "                                  transform=None)\n",
    "\n",
    "test_dataset = SegmentationDataset(image_dir='data/testing',\n",
    "                                   annotations_dir='data/annotations/validation',  # Adjust if needed\n",
    "                                   image_list=test_images,\n",
    "                                   image_ext='.jpg',\n",
    "                                   mask_ext='.png',\n",
    "                                   transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac419d76-a8b8-428a-ac14-b6145d07aeaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training loop code here\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Validation loop code here\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    238\u001b[0m             ]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    # Training loop code here\n",
    "    pass\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    # Validation loop code here\n",
    "    pass\n",
    "\n",
    "for images, masks in test_loader:\n",
    "    # Testing loop code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90f25003-dae3-444d-8b18-e0745c800ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, image_list, image_ext='.jpg', mask_ext='.png', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_list = image_list\n",
    "        self.image_ext = image_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name + self.image_ext)\n",
    "        mask_path = os.path.join(self.annotations_dir, image_name + self.mask_ext)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        mask = self.to_tensor(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Example usage for training and validation datasets\n",
    "train_images, train_categories = parse_train_val_file('data/train.txt')\n",
    "val_images, val_categories = parse_train_val_file('data/val.txt')\n",
    "test_images = parse_test_file('data/test.txt')\n",
    "\n",
    "train_dataset = SegmentationDataset(image_dir='data/images/training',\n",
    "                                    annotations_dir='data/annotations/training',\n",
    "                                    image_list=train_images,\n",
    "                                    image_ext='.jpg',\n",
    "                                    mask_ext='.png',\n",
    "                                    transform=None)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir='data/images/validation',\n",
    "                                  annotations_dir='data/annotations/validation',\n",
    "                                  image_list=val_images,\n",
    "                                  image_ext='.jpg',\n",
    "                                  mask_ext='.png',\n",
    "                                  transform=None)\n",
    "\n",
    "test_dataset = SegmentationDataset(image_dir='data/testing',\n",
    "                                   annotations_dir='data/annotations/validation',  # Adjust if needed\n",
    "                                   image_list=test_images,\n",
    "                                   image_ext='.jpg',\n",
    "                                   mask_ext='.png',\n",
    "                                   transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4f01150-9207-4949-af3d-991ec831bdfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 240, 320] at entry 0 and [3, 256, 256] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training loop code here\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape, masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Validation loop code here\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 240, 320] at entry 0 and [3, 256, 256] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    # Training loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    # Validation loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in test_loader:\n",
    "    # Testing loop code here\n",
    "    print(images.shape, masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b1348bd-2282-4034-95a0-82577abed6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, image_list, image_ext='.jpg', mask_ext='.png', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_list = image_list\n",
    "        self.image_ext = image_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name + self.image_ext)\n",
    "        mask_path = os.path.join(self.annotations_dir, image_name + self.mask_ext)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        image = self.to_tensor(image)\n",
    "        mask = self.to_tensor(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define the transform to resize images and masks to a fixed size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Example usage for training and validation datasets\n",
    "train_images, train_categories = parse_train_val_file('data/train.txt')\n",
    "val_images, val_categories = parse_train_val_file('data/val.txt')\n",
    "test_images = parse_test_file('data/test.txt')\n",
    "\n",
    "train_dataset = SegmentationDataset(image_dir='data/images/training',\n",
    "                                    annotations_dir='data/annotations/training',\n",
    "                                    image_list=train_images,\n",
    "                                    image_ext='.jpg',\n",
    "                                    mask_ext='.png',\n",
    "                                    transform=transform)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir='data/images/validation',\n",
    "                                  annotations_dir='data/annotations/validation',\n",
    "                                  image_list=val_images,\n",
    "                                  image_ext='.jpg',\n",
    "                                  mask_ext='.png',\n",
    "                                  transform=transform)\n",
    "\n",
    "test_dataset = SegmentationDataset(image_dir='data/testing',\n",
    "                                   annotations_dir='data/annotations/validation',  # Adjust if needed\n",
    "                                   image_list=test_images,\n",
    "                                   image_ext='.jpg',\n",
    "                                   mask_ext='.png',\n",
    "                                   transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "424c0b5e-8f0c-4f57-a81a-44d2bbd4c111",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training loop code here\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape, masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Validation loop code here\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[44], line 32\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     30\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(mask)\n\u001b[1;32m---> 32\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(mask)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, mask\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    # Training loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    # Validation loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in test_loader:\n",
    "    # Testing loop code here\n",
    "    print(images.shape, masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83152fed-8f0d-4be0-8e66-e7c3fe8a210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n",
      "C:\\Users\\trish\\AppData\\Local\\Temp\\ipykernel_7360\\3721799417.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(file_path, delim_whitespace=True, header=None, names=[\"image\", \"category\"])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, image_list, image_ext='.jpg', mask_ext='.png', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_list = image_list\n",
    "        self.image_ext = image_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name + self.image_ext)\n",
    "        mask_path = os.path.join(self.annotations_dir, image_name + self.mask_ext)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define the transform to resize images and masks to a fixed size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Example usage for training and validation datasets\n",
    "train_images, train_categories = parse_train_val_file('data/train.txt')\n",
    "val_images, val_categories = parse_train_val_file('data/val.txt')\n",
    "test_images = parse_test_file('data/test.txt')\n",
    "\n",
    "train_dataset = SegmentationDataset(image_dir='data/images/training',\n",
    "                                    annotations_dir='data/annotations/training',\n",
    "                                    image_list=train_images,\n",
    "                                    image_ext='.jpg',\n",
    "                                    mask_ext='.png',\n",
    "                                    transform=transform)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir='data/images/validation',\n",
    "                                  annotations_dir='data/annotations/validation',\n",
    "                                  image_list=val_images,\n",
    "                                  image_ext='.jpg',\n",
    "                                  mask_ext='.png',\n",
    "                                  transform=transform)\n",
    "\n",
    "test_dataset = SegmentationDataset(image_dir='data/testing',\n",
    "                                   annotations_dir='data/annotations/validation',  # Adjust if needed\n",
    "                                   image_list=test_images,\n",
    "                                   image_ext='.jpg',\n",
    "                                   mask_ext='.png',\n",
    "                                   transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b87d4d8-ad4b-43fe-9f8d-994e21c25afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([18, 3, 256, 256]) torch.Size([18, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 256, 256])\n",
      "torch.Size([16, 3, 256, 256]) torch.Size([16, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\trish\\\\OneDrive\\\\Desktop\\\\Segmentation_project\\\\data\\\\testing\\\\ADE_test_00000001.jpg.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Validation loop code here\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape, masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Testing loop code here\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape, masks\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[46], line 24\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, image_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ext)\n\u001b[0;32m     22\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations_dir, image_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_ext)\n\u001b[1;32m---> 24\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\trish\\\\OneDrive\\\\Desktop\\\\Segmentation_project\\\\data\\\\testing\\\\ADE_test_00000001.jpg.jpg'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    # Training loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    # Validation loop code here\n",
    "    print(images.shape, masks.shape)\n",
    "\n",
    "for images, masks in test_loader:\n",
    "    # Testing loop code here\n",
    "    print(images.shape, masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927bd99-e8a3-4c15-91a8-dabb606c5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import SegmentationDataset\n",
    "from data.parse_files import parse_test_file, parse_train_val_file, read_categories_file\n",
    "from models.unet import UNet\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        print(f\"Checkpoint loaded: start_epoch={start_epoch}, best_val_loss={best_val_loss}\")\n",
    "        return start_epoch, best_val_loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at: {checkpoint_path}\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "def train(config, resume_checkpoint=None):\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_images, _ = parse_train_val_file(config['train_images'])\n",
    "    val_images, _ = parse_train_val_file(config['val_images'])\n",
    "    test_images = parse_test_file(config['test_images'])\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(tuple(config['image_size'])),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'training'),\n",
    "                                        annotations_dir=os.path.join(config['annotations_dir'], 'training'),\n",
    "                                        image_list=train_images,\n",
    "                                        image_ext=config['image_ext'],\n",
    "                                        mask_ext=config['mask_ext'],\n",
    "                                        transform=transform)\n",
    "    \n",
    "    val_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'validation'),\n",
    "                                      annotations_dir=os.path.join(config['annotations_dir'], 'validation'),\n",
    "                                      image_list=val_images,\n",
    "                                      image_ext=config['image_ext'],\n",
    "                                      mask_ext=config['mask_ext'],\n",
    "                                      transform=transform)\n",
    "    \n",
    "    test_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'testing'),\n",
    "                                       annotations_dir=os.path.join(config['annotations_dir'], 'validation'),  # Adjust if needed\n",
    "                                       image_list=test_images,\n",
    "                                       image_ext=config['image_ext'],\n",
    "                                       mask_ext=config['mask_ext'],\n",
    "                                       transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = UNet(num_classes=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    model_dir = \"saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    checkpoint_interval = 5  # Save a checkpoint every 5 epochs (you can adjust this)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    if resume_checkpoint:\n",
    "        start_epoch, best_val_loss = load_checkpoint(model, optimizer, resume_checkpoint)\n",
    "\n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{config['epochs']} - Training\") as pbar:\n",
    "            for images, masks in train_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                pbar.update(1)\n",
    "        \n",
    "        model.eval()\n",
    "        # Validation loop with progress bar\n",
    "        with tqdm(total=len(val_loader), desc=f\"Epoch {epoch+1}/{config['epochs']} - Validation\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "                    pbar.set_postfix(loss=loss.item())\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(model_dir, f\"unet_checkpoint_epoch_{epoch + 1}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = os.path.join(model_dir, \"unet_best.pth\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(model_dir, \"unet_final.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"config/default_config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    resume_checkpoint = \"saved_models/unet_checkpoint_epoch_10.pth\"  # Update with the checkpoint path if resuming\n",
    "    train(config, resume_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567645fd-321e-4f3b-bb12-1ecc62600e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import SegmentationDataset\n",
    "from data.parse_files import parse_test_file, parse_train_val_file, read_categories_file\n",
    "from models.unet import UNet\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        print(f\"Checkpoint loaded: start_epoch={start_epoch}, best_val_loss={best_val_loss}\")\n",
    "        return start_epoch, best_val_loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at: {checkpoint_path}\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "def train(config, resume_checkpoint=None):\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_images, _ = parse_train_val_file(config['train_images'])\n",
    "    val_images, _ = parse_train_val_file(config['val_images'])\n",
    "    test_images = parse_test_file(config['test_images'])\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(tuple(config['image_size'])),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'training'),\n",
    "                                        annotations_dir=os.path.join(config['annotations_dir'], 'training'),\n",
    "                                        image_list=train_images,\n",
    "                                        image_ext=config['image_ext'],\n",
    "                                        mask_ext=config['mask_ext'],\n",
    "                                        transform=transform)\n",
    "    \n",
    "    val_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'validation'),\n",
    "                                      annotations_dir=os.path.join(config['annotations_dir'], 'validation'),\n",
    "                                      image_list=val_images,\n",
    "                                      image_ext=config['image_ext'],\n",
    "                                      mask_ext=config['mask_ext'],\n",
    "                                      transform=transform)\n",
    "    \n",
    "    test_dataset = SegmentationDataset(image_dir=os.path.join(config['image_dir'], 'testing'),\n",
    "                                       annotations_dir=os.path.join(config['annotations_dir'], 'validation'),  # Adjust if needed\n",
    "                                       image_list=test_images,\n",
    "                                       image_ext=config['image_ext'],\n",
    "                                       mask_ext=config['mask_ext'],\n",
    "                                       transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = UNet(num_classes=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    model_dir = \"saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    checkpoint_interval = 5  # Save a checkpoint every 5 epochs (you can adjust this)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    if resume_checkpoint:\n",
    "        start_epoch, best_val_loss = load_checkpoint(model, optimizer, resume_checkpoint)\n",
    "\n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{config['epochs']} - Training\") as pbar:\n",
    "            for images, masks in train_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                pbar.update(1)\n",
    "        \n",
    "        model.eval()\n",
    "        # Validation loop with progress bar\n",
    "        with tqdm(total=len(val_loader), desc=f\"Epoch {epoch+1}/{config['epochs']} - Validation\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "                    pbar.set_postfix(loss=loss.item())\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(model_dir, f\"unet_checkpoint_epoch_{epoch + 1}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss  # Correctly save the best validation loss so far\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = os.path.join(model_dir, \"unet_best.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, best_model_path)\n",
    "            print(f\"Best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(model_dir, \"unet_final.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"config/default_config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    resume_checkpoint = None  # Update with the checkpoint path if resuming\n",
    "    train(config, resume_checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
